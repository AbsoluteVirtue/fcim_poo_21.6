# Data-oriented design
Развитие сферы реляционных баз данных дало толчок к появлению набора специфических техник программирования, которые принято называть общим термином "проектирование, ориентированное на данные". Необходимость такого подхода в программировании можно выразить цитатой Никлауса Вирта,
> Программное обеспечение становится медленнее с большей скоростью, чем процессоры становятся быстрее.

Ориентинование на данные,в первую очередь, стало популярным среди разработчиков систем, работающих "реальном времени" -- т.е. это такие системы, для которых отклик на любые события должен происходить за очень короткий (измеряемый в миллисекундах) промежуток времени. Такими системами являются, например, операционные системы, работающие на борту самолета, системы управления баллистическими ракетами, компьютерные игры и т.п.
## Быстродействие программ
Для таких систем есть несколько критически важных свойств, которыми они должны обладать. Ниже приведена неполная таблица различных операций на современных процессорах.

|операция|кол-во тактов|секунды
|--|--|--
|простейшая операция на регистрах (add, or, move)|до 1|10<sup>-9</sup>
|"угаданный" if|1-2|
|сложение векторов|1-3|
|умножение|1-7|
|чтение из кэша первого уровня|3-4|
|чтение из кэша второго уровня|10-12|10<sup>-8</sup>
|"неугаданный" else|10-20|
|деление векторов|10-70|
|целочисленное деление|15-40|
|вызов функции в языке С|20-50|
|вызов виртуальной функции в С++|30-60|
|чтение из кэша третьего уровня|30-70|
|чтение из оперативной памяти|100-150|10<sup>-7</sup>
|динамическое выделение памяти для маленьких объектов|200-500|
|системный вызов ядра ОС (может случиться при динамическом выделении)|1000-1500|10<sup>-6</sup>
|переключение контекста потоков|2000|
|пойманное "исключение"|3000-10000|10<sup>-5</sup>

Это приблизительные значения, и приводятся они только для того, чтобы отметить, что некоторые операции занимают на порядки больше времени, чем простейшие операции. Из этого можно сделать вывод, что арифметические операции, например, эффективнее, чем сохранение их результата в переменную, что может не быть очевидным. Динамическое выделение памяти может занять 1000 циклов процессора, поэтому если память выделяется в цикле несколько раз, это значительно замедлит систему. И отсюда же понятно, почему разработчики С++ не советуют использовать исключения там, где это не является критической необходимостью.

К этому можно добавить тот факт, что процессоры современных архитектур быстрее уже не станут, так как единственное, что можно в них повышать, это количество транзисторов-ядер, а значит выигрыш в производительности может быть достигнут только засчет параллельного программирования. Поэтому так важно внимательно следить за тем, сколько памяти потребляет программа во время работы, сколько данных одновременно передается на процессор, и какие именно данные. Из таблицы выше видно, что самые быстрые операции -- те, которые могут использовать данные из кэша первого или второго уровня. Причем, емкость этих кэшей сильно ограничена. И напротив, работа с основной памятью занимает много времени, а большинство "традиционных" техник программирования предполагает неограниченное выделение объектов в основной памяти.
## Трюки для ускорения программ
До недавних пор компиляторы языков программирования вроде С и С++ были сравнительно простыми и не делали с исходным кодом тех вещей, которые сегодня делаются автоматически (например код, написанный программистом, может быть удален компилятором или сильно модифицирован, если компилятор определит, что это положительно повлияет на работу программы). В прошлом программисты вынуждены были прибегать к различным хитростям для того, чтобы выжать из процессора все, что можно.

	long i;
	float x2, y;
	const float threehalfs = 1.5F;
	x2 = number * 0.5F;
	y  = number;
	i  = * ( long * ) &y;
	i  = 0x5f3759df - ( i >> 1 ); 
	y  = * ( float * ) &i;
	y  = y * ( threehalfs - ( x2 * y * y ) );
Выше приведен пример алгоритма взятия обратного квадратного корня числа, который используется для нормализации векторов в различных системах. Реализация интересна тем, что использует метод Ньютона, причем, применяется побитовая арифметика вместо умножения и деления. Двадцать лет назад использование этого алгоритма позволило сэкономить примерно 30 тактов процессора. Сегодня писать такой алгоритм смысла не имеет, так как компилятор содержит правила преобразования кода, которые подбирают лучшую комбинацию инструкций для каждого случая.

Например, умножение целых чисел принято реализовывать через сложение и побитовые сдвиги результата, если число является степенью двойки.

    // x * 522
    int x;
    x = (x << 9) // 512
      + (x << 3) // 8
      + (x << 1) // 2
    ;
Несмотря на то, что двадцать лет назад такое решение было бы оптимальным, компилятор gcc удалит весь этот код и подставит другую реализацию: imul eax, edi, 522. То есть, он заменяет "эффективные" операции сложения на одну операцию умножения, т.к. на современных архитектурах одна операция умножения сравнительно быстрее цепочки операций сложения.
## Работа с памятью
Примеры выше показали, что бороться за производительность в современных системах не так практично. Комбинация передовых процессорных команд и современных стратегий оптимизации, применяемых в компиляторах, делает последующую оптимизацию отдельных инструкций кода ненужной. На первый план выходят проектирование алгоритмов и композиция данных в структурах.

    int a;  struct { int a; };               // 4, 4
    bool b; struct { bool b; };              // 1, 1
    union  { int a; bool b; }                // 4, 4
    struct { int a; bool b; }                // 4, 8
    struct { int a; double d; int b; }         // 8, 24
    struct { int a; int b; double d; }         // 8, 16
    struct { int a; int b; int *p; bool b; } // 8, 24
Пример выше описывает использование памяти различными структурами в языке С. Здесь важно отметить, что если обычные переменные занимают строго столько памяти, сколько определено платформой (например, 4 байта для int, 1 байт для bool), для структур правила немного другие. Структура должна быть выровнена по самому большому полю, поэтому тип данных, комбинирующий типы int * bool, занимает не 5 байтов, как можно было бы подумать, а 8 (лишние три байта просто не могут быть использованы). По этой же причине структура int * double * int занимает больше места, чем int * int * double. В обоих случаях выравнивание будет по 8 байтов, но если наибольший тип "зажат" между двумя меньшими типами (4-8-4), система будет вынуждена выделять места достаточно под три 8-байтовых значения, чтобы сохранить выравнивание. Если же два поля int идут одно за другим (4-4-8), они оба помещаются в 8 байтов сразу за полем типа double.

Из этого простого правила можно вывести на удивление много правил, которые позволяют значительно сократить количество потребляемой программой памяти.
### Кодирование информации вне структуры
В качестве первого примера будет рассмотрена структура,

    struct s { int a; bool b; };
с помощью которой необходимо объявить массив из 100 объектов такого типа.

    s arraylist[100] = {};
Если проверить размер этого массива, можно ожидать, что он составит 800 байтов ((4 + 4) * 100).